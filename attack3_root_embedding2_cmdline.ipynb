{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.is_available()  # 如果返回True，说明PyTorch可以看到GPU\n",
    "torch.cuda.device_count()  # 返回GPU的数量\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# # 获取当前进程，设置进程亲和性（假设你想使用CPU 17）\n",
    "p = psutil.Process(os.getpid())\n",
    "p.cpu_affinity([17])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>actorID</th>\n",
       "      <th>hostname</th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>objectID</th>\n",
       "      <th>pid</th>\n",
       "      <th>ppid</th>\n",
       "      <th>principal</th>\n",
       "      <th>properties</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>actorname</th>\n",
       "      <th>objectname</th>\n",
       "      <th>phrase</th>\n",
       "      <th>command_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>973155e1-f6bd-4bfb-bad3-85e2692e1bb4</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>34b7a8cc-80c7-43ad-bfd9-103d6284526b</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>aa6c4baf-4bf3-478f-9361-9de472373a63</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>d40ba2ed-8a21-4f9c-b94c-46dfef05c1d5</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>f811ff55-bb94-42c9-b70e-fd0ac3567342</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>5b21e4f6-f3aa-42b4-b9a0-2316a4883265</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>4dcda1cc-d12f-48e4-8486-a6bf4c566477</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>ae5fd4c4-d9e5-43e7-a087-05f969923224</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>b368091e-59ca-43fe-ae97-3ac0b671afd0</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>c037836e-aeca-4356-900d-59e667e24b53</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    action                               actorID                    hostname  \\\n",
       "0  inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "1  inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "2  inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "3  inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "4  inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "\n",
       "                                     id object  \\\n",
       "0  973155e1-f6bd-4bfb-bad3-85e2692e1bb4   FLOW   \n",
       "1  aa6c4baf-4bf3-478f-9361-9de472373a63   FLOW   \n",
       "2  f811ff55-bb94-42c9-b70e-fd0ac3567342   FLOW   \n",
       "3  4dcda1cc-d12f-48e4-8486-a6bf4c566477   FLOW   \n",
       "4  b368091e-59ca-43fe-ae97-3ac0b671afd0   FLOW   \n",
       "\n",
       "                               objectID  pid  ppid  \\\n",
       "0  34b7a8cc-80c7-43ad-bfd9-103d6284526b  864   560   \n",
       "1  d40ba2ed-8a21-4f9c-b94c-46dfef05c1d5  864   560   \n",
       "2  5b21e4f6-f3aa-42b4-b9a0-2316a4883265  864   560   \n",
       "3  ae5fd4c4-d9e5-43e7-a087-05f969923224  864   560   \n",
       "4  c037836e-aeca-4356-900d-59e667e24b53  864   560   \n",
       "\n",
       "                      principal  \\\n",
       "0  NT AUTHORITY\\NETWORK SERVICE   \n",
       "1  NT AUTHORITY\\NETWORK SERVICE   \n",
       "2  NT AUTHORITY\\NETWORK SERVICE   \n",
       "3  NT AUTHORITY\\NETWORK SERVICE   \n",
       "4  NT AUTHORITY\\NETWORK SERVICE   \n",
       "\n",
       "                                          properties  tid           timestamp  \\\n",
       "0  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "1  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "2  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "3  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "4  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "\n",
       "                                           actorname        objectname  \\\n",
       "0  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "2  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "3  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "4  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "\n",
       "                                              phrase command_line  \n",
       "0  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "1  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "2  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "3  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "4  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# /home/cch/TraceSentinel/data/optc/benign/20-23Sep19/concat_SysClient0051.systemia.com.txt\n",
    "with open(\"../data/optc/benign/20-23Sep19/concat_SysClient0051.systemia.com.txt\", 'r') as file:\n",
    "    content = [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "\n",
    "def Extract_Semantic_Info(event):\n",
    "    object_type = event['object']\n",
    "    properties = event['properties']\n",
    "\n",
    "    label_mapping = {\n",
    "        \"PROCESS\": ('parent_image_path', 'image_path'),\n",
    "        \"FILE\": ('image_path', 'file_path'),\n",
    "        \"FLOW\": ('image_path', 'dest_ip', 'dest_port', 'direction'),\n",
    "        'MODULE': ('image_path', 'module_path')\n",
    "    }\n",
    "\n",
    "    label_keys = label_mapping.get(object_type, None)\n",
    "    if label_keys:\n",
    "        labels = [properties.get(key) for key in label_keys]\n",
    "        # print(labels)\n",
    "        if all(labels):\n",
    "            if object_type==\"PROCESS\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], labels[1]\n",
    "                event[\"command_line\"] = properties.get(\"command_line\", \"None\")\n",
    "                # event[\"command_line\"] = properties.get(\"command_line\", \"None\") if len(properties.get(\"command_line\", \"None\")) > 70 else properties.get(\"command_line\", \"None\")\n",
    "                # event[\"command_line\"] = labels[2] if len(labels[2]) > 70 else labels[2]\n",
    "            elif object_type==\"FILE\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], labels[1]\n",
    "            elif object_type==\"FLOW\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], '-'.join(labels[1:3])\n",
    "                event['action'] = labels[3]\n",
    "            elif object_type == \"MODULE\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], labels[1]\n",
    "            else:\n",
    "                pass\n",
    "            return event\n",
    "    return None\n",
    "\n",
    "\n",
    "def Sentence_Construction(entry):\n",
    "    action = entry[\"action\"]\n",
    "    properties = entry['properties']\n",
    "    object_type = entry['object']\n",
    "    phrase=[]\n",
    "    try:\n",
    "        if object_type==\"PROCESS\":\n",
    "            phrase.append(properties.get(\"parent_image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(properties.get(\"command_line\", \"N/A\"))\n",
    "        elif object_type==\"FILE\":\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            phrase.append(properties.get(\"file_path\", \"N/A\"))\n",
    "        elif object_type==\"FLOW\":\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            dest_ip = str(properties.get(\"dest_ip\", \"N/A\"))\n",
    "            dest_port = str(properties.get(\"dest_port\", \"N/A\"))\n",
    "            phrase.append(dest_ip+'-'+dest_port)\n",
    "            # phrase.append(properties.get(\"src_ip\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"src_port\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"dest_ip\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"dest_port\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"direction\", \"N/A\"))\n",
    "        elif object_type==\"MODULE\":\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            phrase.append(properties.get(\"module_path\", \"N/A\"))\n",
    "        else:\n",
    "            pass\n",
    "    except KeyError:\n",
    "        phrase = []\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def is_valid_entry(entry):\n",
    "    valid_objects = {'PROCESS', 'FILE', 'FLOW', 'MODULE'}\n",
    "    invalid_actions = {'START', 'TERMINATE', 'OPEN'}\n",
    "\n",
    "    object_valid = entry['object'] in valid_objects\n",
    "    action_valid = entry['action'] not in invalid_actions\n",
    "    actor_object_different = entry['actorID'] != entry['objectID']\n",
    "\n",
    "    return object_valid and action_valid and actor_object_different\n",
    "\n",
    "def Traversal_Rules(data):\n",
    "    filtered_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if is_valid_entry(entry):\n",
    "            filtered_data.append(entry)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def transform(text):\n",
    "    labeled_data = [event for event in (Extract_Semantic_Info(x) for x in text) if event]\n",
    "    data = Traversal_Rules(labeled_data)\n",
    "\n",
    "\n",
    "    phrases = [Sentence_Construction(x) for x in data if Sentence_Construction(x)]\n",
    "    for datum, phrase in zip(data, phrases):\n",
    "        datum['phrase'] = phrase\n",
    "\n",
    "    # data = sorted(data, key=lambda x: x.get('timestamp'))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str[:19], format='%Y-%m-%dT%H:%M:%S')\n",
    "    df.sort_values(by='timestamp', inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = transform(content)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_df(df):\n",
    "    # 指定列转换为小写\n",
    "    columns_to_lower = ['action', 'object', 'actorname', 'objectname', 'command_line', 'phrase']\n",
    "    # df[columns_to_lower] = df[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    # 使用apply和lambda函数转换列中的值\n",
    "    df[columns_to_lower] = df[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    # 对'phrase'列中的每个元素（列表中的字符串）应用小写转换\n",
    "    df['phrase'] = df['phrase'].apply(lambda x: [item.lower() if isinstance(item, str) else item for item in x])\n",
    "\n",
    "    node2name = {}\n",
    "    # 首先遍历create事件，提取进程名\n",
    "    extract_process_name = df[(df['object'] == 'process') & (df['action'] == 'create')]\n",
    "    for row in extract_process_name.itertuples():\n",
    "        nodeid = row.objectID\n",
    "        processname = row.objectname\n",
    "        if nodeid not in node2name:\n",
    "            node2name[nodeid] = processname\n",
    "    for row in df[(df['object'] == 'process') & (df['action'] != 'create')].itertuples():\n",
    "        nodeid = row.objectID\n",
    "        processname = row.objectname\n",
    "        if nodeid not in node2name:\n",
    "            node2name[nodeid] = processname\n",
    "    # 使用 map 方法将 node2name 字典中的值应用到objectname列\n",
    "    # fillna(df['objectname']) 保证如果没有对应的 nodeid，在不改变原来的值的情况下保持 objectname 原样\n",
    "    df['objectname'] = df['objectID'].map(node2name).fillna(df['objectname'])\n",
    "    df['actorname'] = df['actorID'].map(node2name).fillna(df['actorname'])\n",
    "\n",
    "\n",
    "    new_path = {}\n",
    "    for row in df[(df['object'] == 'file') & (df['action'] == 'rename')].itertuples():\n",
    "        nodeid = row.objectID\n",
    "        new_pathname = row.properties.get('new_path', 'None')\n",
    "        if nodeid not in new_path:\n",
    "            new_path[nodeid] = new_pathname\n",
    "\n",
    "    df['objectname'] = df['objectID'].map(new_path).fillna(df['objectname'])\n",
    "\n",
    "    # 去除重复事件\n",
    "    df.drop_duplicates(subset=['action', 'actorID', 'objectID', 'object', 'actorname', 'objectname', 'pid', 'ppid'], inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # 根据进程名称和pid生成新的索引\n",
    "    for index, row in df.iterrows():\n",
    "        if row['object'] == 'process':\n",
    "            # 对于 'process' 类型\n",
    "            src_id = stringtomd5(row['actorname'] + str(row['ppid']))\n",
    "            dest_id = stringtomd5(row['objectname'] + str(row['pid']))\n",
    "        elif row['object'] in ['flow', 'file', 'module']:\n",
    "            # 对于 'flow' 或 'file' 类型\n",
    "            src_id = stringtomd5(row['actorname'] + str(row['pid']))\n",
    "            dest_id = stringtomd5(row['objectname'])\n",
    "        else:\n",
    "            # 如果其他类型的 object\n",
    "            src_id = None\n",
    "            dest_id = None\n",
    "        \n",
    "        # 将结果存入新的列\n",
    "        df.at[index, 'src_id'] = src_id\n",
    "        df.at[index, 'dest_id'] = dest_id\n",
    "\n",
    "    # 去除重复事件\n",
    "    df.drop_duplicates(subset=['action', 'src_id', 'dest_id', 'object', 'actorname', 'objectname'], inplace=True)\n",
    "    df = df[df['src_id'] != df['dest_id']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = proprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>actorID</th>\n",
       "      <th>hostname</th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>objectID</th>\n",
       "      <th>pid</th>\n",
       "      <th>ppid</th>\n",
       "      <th>principal</th>\n",
       "      <th>properties</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>actorname</th>\n",
       "      <th>objectname</th>\n",
       "      <th>phrase</th>\n",
       "      <th>command_line</th>\n",
       "      <th>src_id</th>\n",
       "      <th>dest_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>973155e1-f6bd-4bfb-bad3-85e2692e1bb4</td>\n",
       "      <td>flow</td>\n",
       "      <td>34b7a8cc-80c7-43ad-bfd9-103d6284526b</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\device\\harddiskvolume1\\windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\device\\harddiskvolume1\\windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d3870e77b3db2856c576b5dffa2f4e2acb5c0f6e7079bf...</td>\n",
       "      <td>540a960fe00f02eb2e4f3087a04e3c95767d350cfc8fb5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inbound</td>\n",
       "      <td>cec415a7-865f-4714-afa9-610b83d2c2da</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>86755d66-306e-462b-a8d7-276ffb1a1493</td>\n",
       "      <td>flow</td>\n",
       "      <td>359a4a06-0d56-4ac3-8be7-be342a122cc7</td>\n",
       "      <td>864</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': 'ff02::fb', '...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\device\\harddiskvolume1\\windows\\system32\\svcho...</td>\n",
       "      <td>ff02::fb-5353</td>\n",
       "      <td>[\\device\\harddiskvolume1\\windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d3870e77b3db2856c576b5dffa2f4e2acb5c0f6e7079bf...</td>\n",
       "      <td>e9279d5c69ac1643a5aff74e8c61d79ca87c277c8e44b5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inbound</td>\n",
       "      <td>6ce25b35-0a1a-48bb-94d1-5f9b2bc2f96c</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>4e58b42f-3b68-41a0-b5c3-28af6f6117b2</td>\n",
       "      <td>flow</td>\n",
       "      <td>e6ce56f2-30b1-4b2e-8f96-e6f6b19f851f</td>\n",
       "      <td>2256</td>\n",
       "      <td>2044</td>\n",
       "      <td>NT AUTHORITY\\SYSTEM</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '225.0.0.1', ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\device\\harddiskvolume1\\python27\\python.exe</td>\n",
       "      <td>225.0.0.1-5000</td>\n",
       "      <td>[\\device\\harddiskvolume1\\python27\\python.exe, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>039cb93a59264531597071bef7cd397b596ab60cfbc692...</td>\n",
       "      <td>2e5184a3ab6979f2edb0508f3e863f7cb8e52a5a538247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>outbound</td>\n",
       "      <td>5864828d-d5a3-42a0-9f89-39a18bfb80d0</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>3f1494a8-f495-4a91-b280-05c82eb27d73</td>\n",
       "      <td>flow</td>\n",
       "      <td>75dddd54-0389-4e03-961e-ba41cc821f42</td>\n",
       "      <td>2008</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\SYSTEM</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '10.50.5.4', ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:40</td>\n",
       "      <td>\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe</td>\n",
       "      <td>10.50.5.4-9092</td>\n",
       "      <td>[\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4f5e2420dbe8a5cffa4cb8519876b181f710f020dd8a25...</td>\n",
       "      <td>8e27ba967e698756e8244dee6489b8fdf749852835b56b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>outbound</td>\n",
       "      <td>5864828d-d5a3-42a0-9f89-39a18bfb80d0</td>\n",
       "      <td>SysClient0051.systemia.com</td>\n",
       "      <td>e6028c1c-dcdf-4d06-b40f-a426b5aa3906</td>\n",
       "      <td>flow</td>\n",
       "      <td>53996327-b75b-4d06-a087-3bcfbd94cd6f</td>\n",
       "      <td>2008</td>\n",
       "      <td>560</td>\n",
       "      <td>NT AUTHORITY\\SYSTEM</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '10.50.5.14',...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:41</td>\n",
       "      <td>\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe</td>\n",
       "      <td>10.50.5.14-9092</td>\n",
       "      <td>[\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4f5e2420dbe8a5cffa4cb8519876b181f710f020dd8a25...</td>\n",
       "      <td>ea549ac3d509b679ec822064a22a793b1ed64e75d0057c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     action                               actorID                    hostname  \\\n",
       "0   inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "1   inbound  cec415a7-865f-4714-afa9-610b83d2c2da  SysClient0051.systemia.com   \n",
       "2   inbound  6ce25b35-0a1a-48bb-94d1-5f9b2bc2f96c  SysClient0051.systemia.com   \n",
       "3  outbound  5864828d-d5a3-42a0-9f89-39a18bfb80d0  SysClient0051.systemia.com   \n",
       "4  outbound  5864828d-d5a3-42a0-9f89-39a18bfb80d0  SysClient0051.systemia.com   \n",
       "\n",
       "                                     id object  \\\n",
       "0  973155e1-f6bd-4bfb-bad3-85e2692e1bb4   flow   \n",
       "1  86755d66-306e-462b-a8d7-276ffb1a1493   flow   \n",
       "2  4e58b42f-3b68-41a0-b5c3-28af6f6117b2   flow   \n",
       "3  3f1494a8-f495-4a91-b280-05c82eb27d73   flow   \n",
       "4  e6028c1c-dcdf-4d06-b40f-a426b5aa3906   flow   \n",
       "\n",
       "                               objectID   pid  ppid  \\\n",
       "0  34b7a8cc-80c7-43ad-bfd9-103d6284526b   864   560   \n",
       "1  359a4a06-0d56-4ac3-8be7-be342a122cc7   864   560   \n",
       "2  e6ce56f2-30b1-4b2e-8f96-e6f6b19f851f  2256  2044   \n",
       "3  75dddd54-0389-4e03-961e-ba41cc821f42  2008   560   \n",
       "4  53996327-b75b-4d06-a087-3bcfbd94cd6f  2008   560   \n",
       "\n",
       "                      principal  \\\n",
       "0  NT AUTHORITY\\NETWORK SERVICE   \n",
       "1  NT AUTHORITY\\NETWORK SERVICE   \n",
       "2           NT AUTHORITY\\SYSTEM   \n",
       "3           NT AUTHORITY\\SYSTEM   \n",
       "4           NT AUTHORITY\\SYSTEM   \n",
       "\n",
       "                                          properties  tid           timestamp  \\\n",
       "0  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "1  {'acuity_level': '1', 'dest_ip': 'ff02::fb', '...   -1 2019-09-19 17:11:39   \n",
       "2  {'acuity_level': '1', 'dest_ip': '225.0.0.1', ...   -1 2019-09-19 17:11:39   \n",
       "3  {'acuity_level': '1', 'dest_ip': '10.50.5.4', ...   -1 2019-09-19 17:11:40   \n",
       "4  {'acuity_level': '1', 'dest_ip': '10.50.5.14',...   -1 2019-09-19 17:11:41   \n",
       "\n",
       "                                           actorname        objectname  \\\n",
       "0  \\device\\harddiskvolume1\\windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "1  \\device\\harddiskvolume1\\windows\\system32\\svcho...     ff02::fb-5353   \n",
       "2        \\device\\harddiskvolume1\\python27\\python.exe    225.0.0.1-5000   \n",
       "3       \\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe    10.50.5.4-9092   \n",
       "4       \\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe   10.50.5.14-9092   \n",
       "\n",
       "                                              phrase command_line  \\\n",
       "0  [\\device\\harddiskvolume1\\windows\\system32\\svch...          NaN   \n",
       "1  [\\device\\harddiskvolume1\\windows\\system32\\svch...          NaN   \n",
       "2  [\\device\\harddiskvolume1\\python27\\python.exe, ...          NaN   \n",
       "3  [\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe,...          NaN   \n",
       "4  [\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe,...          NaN   \n",
       "\n",
       "                                              src_id  \\\n",
       "0  d3870e77b3db2856c576b5dffa2f4e2acb5c0f6e7079bf...   \n",
       "1  d3870e77b3db2856c576b5dffa2f4e2acb5c0f6e7079bf...   \n",
       "2  039cb93a59264531597071bef7cd397b596ab60cfbc692...   \n",
       "3  4f5e2420dbe8a5cffa4cb8519876b181f710f020dd8a25...   \n",
       "4  4f5e2420dbe8a5cffa4cb8519876b181f710f020dd8a25...   \n",
       "\n",
       "                                             dest_id  \n",
       "0  540a960fe00f02eb2e4f3087a04e3c95767d350cfc8fb5...  \n",
       "1  e9279d5c69ac1643a5aff74e8c61d79ca87c277c8e44b5...  \n",
       "2  2e5184a3ab6979f2edb0508f3e863f7cb8e52a5a538247...  \n",
       "3  8e27ba967e698756e8244dee6489b8fdf749852835b56b...  \n",
       "4  ea549ac3d509b679ec822064a22a793b1ed64e75d0057c...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['object'] != \"module\"]\n",
    "read_df = df[df['action'] == 'read']\n",
    "write_actions = ['write', 'modify', 'create', 'rename']\n",
    "\n",
    "# 筛选出写操作的行，并排除由 system 和 dwm.exe 修改的文件\n",
    "# write_df = df[(df['action'].isin(write_actions)) & (~df['actorname'].isin(['system', 'dwm.exe']))]\n",
    "sys_dwm = df[df['actorname'].isin(['system', 'dwm.exe'])]['dest_id']\n",
    "\n",
    "write_df = df[df['action'].isin(write_actions)]\n",
    "\n",
    "\n",
    "\n",
    "# write_object_ids = set(write_df['dest_id'])\n",
    "write_object_ids = set(write_df['dest_id']) - set(sys_dwm)\n",
    "readonly_df = read_df[~read_df['dest_id'].isin(write_object_ids)]\n",
    "readonly_df_ids = readonly_df['dest_id']\n",
    "df = df[~df['dest_id'].isin(readonly_df_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从df中去除名为\\device\\harddiskvolume1\\windows\\system32\\svchost.exe和\\device\\harddiskvolume1\\windows\\system32\\conhost.exe的节点\n",
    "exclude_values = [\n",
    "    '\\device\\harddiskvolume1\\windows\\system32\\svchost.exe',\n",
    "    '\\device\\harddiskvolume1\\windows\\system32\\conhost.exe',\n",
    "    'svchost.exe',\n",
    "    'consent.exe'\n",
    "]\n",
    "\n",
    "# 过滤掉包含指定值的行\n",
    "df = df[~df['actorname'].isin(exclude_values) & ~df['objectname'].isin(exclude_values)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到所有 create 操作的父进程和子进程\n",
    "parent_child_df = df[(df['action'] == 'create') & (df['object'] == 'process')]\n",
    "\n",
    "# 提取所有父进程和子进程的 ID\n",
    "parent_ids = set(parent_child_df['src_id'])  # 父进程 ID\n",
    "child_ids = set(parent_child_df['dest_id'])   # 子进程 ID\n",
    "\n",
    "# 所有相关的进程 ID（即有父进程或有子进程的）\n",
    "related_ids = parent_ids.union(child_ids)\n",
    "\n",
    "# 从 df 中移除既没有父进程也没有子进程的节点\n",
    "df = df[df['src_id'].isin(related_ids) | df['dest_id'].isin(related_ids)]\n",
    "\n",
    "# 重置索引\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉没有cmdline的进程节点：\n",
    "df = df[~((df['object'] == 'process') & (df['command_line'] == 'none'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 假设 df 是你的 DataFrame\n",
    "# 1. 筛选 object 列为 'process' 的行\n",
    "df_process = df[df['object'] == 'process']\n",
    "\n",
    "# 2. 创建一个无向图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 3. 添加边 (src_id, dest_id) 到图中\n",
    "for _, row in df_process.iterrows():\n",
    "    G.add_edge(row['src_id'], row['dest_id'])\n",
    "\n",
    "# 4. 获取图中的所有连通子图\n",
    "connected_components = list(nx.connected_components(G))\n",
    "\n",
    "# 5. 过滤掉节点数小于 2 的子图\n",
    "filtered_components = [comp for comp in connected_components if len(comp) > 2]\n",
    "\n",
    "# 6. 获取所有保留的节点\n",
    "valid_nodes = set(node for component in filtered_components for node in component)\n",
    "\n",
    "# 7. 根据 valid_nodes 过滤原 DataFrame 中的行\n",
    "# df_filtered = df_process[df_process['src_id'].isin(valid_nodes) & df_process['dest_id'].isin(valid_nodes)]\n",
    "\n",
    "# 结果：df_filtered 就是筛选后的 DataFrame\n",
    "df = df[(df['src_id'].isin(valid_nodes) | df['dest_id'].isin(valid_nodes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read      202828\n",
       "modify     22282\n",
       "write      19839\n",
       "create     14812\n",
       "delete     12241\n",
       "rename      2168\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df['object'] == 'file']['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module     439922\n",
       "file       274170\n",
       "flow        61960\n",
       "process     19535\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['object'] != \"module\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file       274170\n",
       "flow        61960\n",
       "process     19535\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_df = df[df['action'] == 'read']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_actions = ['write', 'modify', 'create', 'rename']\n",
    "# write_df = df[df['action'].isin(write_actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_object_ids = set(write_df['dest_id'])\n",
    "# readonly_df = read_df[~read_df['dest_id'].isin(write_object_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readonly_df_ids = readonly_df['dest_id']\n",
    "# df = df[~df['dest_id'].isin(readonly_df_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file       193448\n",
       "flow        61960\n",
       "process     19535\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('../trained_weights/optc/cmdline_attack3_w2v.model')\n",
    "        self.epoch += 1\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "\n",
    "def prepare_sentences(df):\n",
    "    nodes = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for key in ['src_id', 'dest_id']:\n",
    "            node_id = row[key]\n",
    "            nodes.setdefault(node_id, []).extend(row['phrase'])\n",
    "    return list(nodes.values())\n",
    "\n",
    "def train_word2vec_model(df):\n",
    "    phrases = prepare_sentences(df)\n",
    "\n",
    "    logger = EpochLogger()\n",
    "    saver = EpochSaver()\n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=64, window=5, min_count=1, workers=8, epochs=50, callbacks=[saver, logger])\n",
    "\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x79527fdd03d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word2vec_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程见optc_parser3\n",
    "word2vec_weights = '../trained_weights/optc/cmdline_attack3_w2v.model'\n",
    "w2vmodel = Word2Vec.load(word2vec_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inbound     41125\n",
       "create      17069\n",
       "read        14898\n",
       "outbound    10785\n",
       "modify      10264\n",
       "write        8289\n",
       "delete       6796\n",
       "rename       1220\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def infer(word):\n",
    "    if word in  w2vmodel.wv:\n",
    "        word_embeddings = w2vmodel.wv[word]\n",
    "    else:\n",
    "        word_embeddings = np.zeros(64)\n",
    "\n",
    "    output_embedding = torch.tensor(word_embeddings, dtype=torch.float)\n",
    "    output_embedding = output_embedding.detach().cpu().numpy()\n",
    "    return output_embedding\n",
    "\n",
    "\n",
    "def Featurize(df):\n",
    "    # dummies = {'process': 0, 'flow': 1, 'file': 2, 'module': 3}\n",
    "    dummies = {'process': 0, 'flow': 1, 'file': 2}\n",
    "\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    lblmap = {}\n",
    "    neimap = {}\n",
    "    edges = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        actor_id, object_id = row['src_id'], row[\"dest_id\"]\n",
    "        object_type = row['object']\n",
    "\n",
    "        if actor_id not in nodes:\n",
    "            nodes[actor_id] = row['actorname']\n",
    "        if object_id not in nodes:\n",
    "            nodes[object_id] = row['objectname']\n",
    "\n",
    "        # nodes.setdefault(actor_id, []).extend(row['phrase'])\n",
    "        # nodes.setdefault(object_id, []).extend(row['phrase'])\n",
    "\n",
    "        labels[actor_id] = dummies.get('process', -1)\n",
    "        labels[object_id] = dummies.get(object_type, -1)\n",
    "\n",
    "        if actor_id not in lblmap:\n",
    "            lblmap[actor_id] = row['actorname']\n",
    "        if object_id not in lblmap:\n",
    "            lblmap[object_id] = row['objectname']\n",
    "\n",
    "        neimap.setdefault(actor_id, set()).add(row['objectname'])\n",
    "        neimap.setdefault(object_id, set()).add(row['actorname'])\n",
    "\n",
    "        edge_type = row['action']\n",
    "        edges.append((actor_id, object_id, edge_type))\n",
    "\n",
    "    features, feat_labels, edge_index = [], [], [[], []]\n",
    "    node_index = {}\n",
    "\n",
    "    for node, nodename in nodes.items():\n",
    "        features.append(infer(nodename))\n",
    "        feat_labels.append(labels[node])\n",
    "        node_index[node] = len(features) - 1\n",
    "\n",
    "    for src, dst, action in edges:\n",
    "        if action=='read' or action=='inbound':\n",
    "            edge_index[0].append(node_index[dst])\n",
    "            edge_index[1].append(node_index[src])\n",
    "        elif action=='create':\n",
    "            edge_index[0].append(node_index[src])\n",
    "            edge_index[1].append(node_index[dst])\n",
    "            edge_index[0].append(node_index[dst])\n",
    "            edge_index[1].append(node_index[src])\n",
    "        else:\n",
    "            edge_index[0].append(node_index[src])\n",
    "            edge_index[1].append(node_index[dst])\n",
    "\n",
    "    mapp = list(node_index.keys())\n",
    "\n",
    "    return features, np.array(feat_labels), edge_index, mapp, lblmap, neimap\n",
    "\n",
    "nodes,labels,edges,mapp,lbl,nemap = Featurize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44933, 44933, 44933)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes),len(labels),len(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个字典，存储每个节点对应的特征向量\n",
    "nodes_vec = {}\n",
    "for i, j in zip(mapp, nodes):\n",
    "    nodes_vec[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交换源节点和目标节点，因为信息汇聚是由源节点汇聚到目标节点\n",
    "# edges[0], edges[1] = edges[1], edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taskkill  /f /im onedrive.exe'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['object'] == 'process']['command_line'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.5090013 ,  0.495144  , -1.5924969 , -3.185587  , -4.221437  ,\n",
       "        1.8156747 ,  2.7987738 , -3.1541677 , -4.0099587 ,  1.4869906 ,\n",
       "        3.1894996 , -0.6768038 ,  1.6493262 , -1.5898938 ,  1.9805825 ,\n",
       "       -4.999961  ,  5.0430083 ,  2.3479817 , -0.07772961,  0.3344564 ,\n",
       "       -1.4188371 ,  0.38762727, -2.5812497 , -2.0923703 , -0.51867294,\n",
       "        0.22688112, -1.5684015 ,  0.8538703 , -2.816361  , -1.3767241 ,\n",
       "        1.2930183 ,  0.3037087 , -2.9468863 ,  1.70543   , -1.3664804 ,\n",
       "       -1.1691194 ,  3.828428  ,  0.36555034,  0.50062925,  4.155876  ,\n",
       "        3.0799754 ,  0.7041244 , -0.64227194,  0.41443297, -2.8001711 ,\n",
       "       -0.84145224, -1.1332116 ,  1.0092577 ,  1.0817299 ,  2.6131704 ,\n",
       "        0.3606389 ,  1.1682682 , -2.542237  , -1.883746  ,  2.6413238 ,\n",
       "        2.8002207 , -6.2621565 , -1.6741657 ,  1.8254343 , -0.4466301 ,\n",
       "        5.10689   , -0.2982849 ,  2.904278  ,  2.743053  ], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('schtasks  /query /tn \"kickoffautologinrandom\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从df中去除名为\\device\\harddiskvolume1\\windows\\system32\\svchost.exe和\\device\\harddiskvolume1\\windows\\system32\\conhost.exe的节点\n",
    "# exclude_values = [\n",
    "#     '\\device\\harddiskvolume1\\windows\\system32\\svchost.exe',\n",
    "#     '\\device\\harddiskvolume1\\windows\\system32\\conhost.exe',\n",
    "#     'svchost.exe',\n",
    "#     'consent.exe'\n",
    "# ]\n",
    "\n",
    "# # 过滤掉包含指定值的行\n",
    "# df = df[~df['actorname'].isin(exclude_values) & ~df['objectname'].isin(exclude_values)]\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lowest_ancestor_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_lineage = df[(df['action'] == 'create') & (df['object'] == 'process')]\n",
    "\n",
    "# 记录每个进程的父进程及其时间戳和 cmdline 信息\n",
    "process_parents = {}\n",
    "\n",
    "# 进程谱系树生成\n",
    "import networkx as nx\n",
    "\n",
    "g = nx.MultiDiGraph()\n",
    "node_map = {}\n",
    "# count_node = 0\n",
    "for index, row in df_process_lineage.iterrows():\n",
    "    src, source_name, dst, dstname, etype, object_type = row['src_id'], row[\"actorname\"],  row['dest_id'], row['objectname'], row['action'], row['object']\n",
    "    command_line = row['command_line']\n",
    "    if src not in node_map:\n",
    "        node_map[src] = src\n",
    "        g.add_node(src, type=\"process\", pname=source_name)\n",
    "    \n",
    "    if dst not in node_map:\n",
    "        node_map[dst] = dst\n",
    "        if source_name==dstname:\n",
    "            g.add_node(dst, type=object_type, pname=dstname)\n",
    "        else:\n",
    "            g.add_node(dst, type=object_type, pname=dstname)\n",
    "    \n",
    "    if g.has_edge(node_map[dst], node_map[src]):\n",
    "        pass\n",
    "    else:\n",
    "        if g.has_edge(node_map[src], node_map[dst]):\n",
    "            pass\n",
    "        elif etype=='create':\n",
    "            g.add_edge(node_map[src], node_map[dst], type=etype, commandline=command_line)\n",
    "        else:\n",
    "            pass\n",
    "    if dst not in process_parents:\n",
    "        process_parents[dst] = []\n",
    "    process_parents[dst].append((src, command_line))\n",
    "\n",
    "def find_lowest_common_ancestor(graph, target_pid):\n",
    "    \"\"\"\n",
    "    查找某个进程的具有多个子进程的最低祖先。\n",
    "    :param graph: NetworkX图对象\n",
    "    :param target_pid: 目标进程ID\n",
    "    :return: 具有多个子进程的最低祖先进程ID\n",
    "    \"\"\"\n",
    "    # 获取目标进程的父进程\n",
    "    def get_parent(pid):\n",
    "        parents = process_parents.get(pid, [])\n",
    "        if parents:\n",
    "            return parents[0][0]  # 假设每个进程有一个父进程\n",
    "        return None\n",
    "    \n",
    "    # 遍历并查找具有多个子进程的父进程\n",
    "    def has_multiple_children(pid):\n",
    "        children = list(graph.successors(pid))\n",
    "        return len(children) > 1  # 如果父进程有多个子进程\n",
    "    \n",
    "    # 从目标进程开始，逐步向上遍历父进程\n",
    "    current_pid = target_pid\n",
    "    \n",
    "    while True:\n",
    "        parent_pid = get_parent(current_pid)\n",
    "        if parent_pid is None:\n",
    "            return None  # 找不到父进程，说明已经是根节点\n",
    "        if has_multiple_children(parent_pid):\n",
    "            # return parent_pid\n",
    "            return current_pid  # 找到具有多个子进程的最低祖先\n",
    "        current_pid = parent_pid  # 向上查找父进程\n",
    "\n",
    "# 存储每个进程的最低祖先\n",
    "process_lca_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "# 遍历所有进程，查找其最低祖先并存储\n",
    "for node in g.nodes:\n",
    "    lca = find_lowest_common_ancestor(g, node)\n",
    "    if lca is not None:\n",
    "        process_lca_dict[node] = process_parents.get(lca, 'meiyou')[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7097"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49's lowest ancestor with multiple children: c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49\n"
     ]
    }
   ],
   "source": [
    "# 示例使用\n",
    "process_id = 'c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49'  # 例如，查找进程 '1006' 的最低祖先\n",
    "ancestor = find_lowest_common_ancestor(g, process_id)\n",
    "if ancestor:\n",
    "    print(f\"Process {process_id}'s lowest ancestor with multiple children: {ancestor}\")\n",
    "else:\n",
    "    print(f\"Process {process_id} has no ancestor with multiple children.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\windows\\system32\\cmd.exe /c \"schtasks /delete /tn \"kickoffautologinrandom\" /f\"\n"
     ]
    }
   ],
   "source": [
    "print(process_lca_dict.get('c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果一个进程没有父进程，那么他的根节点是open他的进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(process_lca_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GNNWithRootEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, process_lca_dict, node_id_to_hash, node_hash_to_vec):\n",
    "        super(GNNWithRootEmbedding, self).__init__()\n",
    "        \n",
    "        self.node_hash_to_vec = node_hash_to_vec # 存储节点的uuid到其初始向量的映射，其中uuid是hash值\n",
    "        self.node_id_to_hash = node_id_to_hash # 存储节点的batch.n_id到节点uuid的映射，其中batch.n_id的数值，例如1，2，3，其中uuid是hash值\n",
    "        self.process_lca_dict = process_lca_dict  # 存储节点与其根节点ID的映射\n",
    "        self.num_features = num_features  # 节点特征维度\n",
    "        \n",
    "        # 第一层图卷积，输入维度为特征 + 根节点特征维度\n",
    "        self.conv1 = SAGEConv(num_features, 128, normalize=True)  # 用于邻居节点特征的聚合\n",
    "        # 第二层图卷积\n",
    "        self.conv2 = SAGEConv(128, 64, normalize=True)\n",
    "        \n",
    "        # 用于根节点特征的变换矩阵W2\n",
    "        self.root_transform = nn.Linear(num_features, 128)\n",
    "        # 用于根节点特征的变换矩阵W3\n",
    "        self.root_transform2 = nn.Linear(128, 64)\n",
    "\n",
    "        self.linear = nn.Linear(64, 3)\n",
    "    \n",
    "    def root_embedding(self, node_ids):\n",
    "        \"\"\"\n",
    "        通过根节点映射字典，直接获取根节点的特征。\n",
    "        :param node_ids: 当前批次中的节点ID\n",
    "        :return: 返回每个节点对应的根节点嵌入特征\n",
    "        \"\"\"\n",
    "        # 获取节点对应的hash值\n",
    "        node_hash = [self.node_id_to_hash[node_id] for node_id in node_ids]\n",
    "        # 获取节点对应的根节点特征（通过process_lca_dict获得根节点ID）\n",
    "        root_ids = [self.process_lca_dict.get(node_id, \"meiyougengjiedian\") for node_id in node_hash]\n",
    "        \n",
    "        # 在这里你可以调用任意方法返回根节点的特征\n",
    "        # 假设根节点特征向量已经通过某种方式计算并存储，返回对应的特征\n",
    "        # get_origin_embedding 函数假设返回根节点的嵌入特征\n",
    "        # return torch.stack([torch.tensor(self.node_hash_to_vec.get(root_id, torch.zeros(self.num_features))) for root_id in root_ids], dim=0)\n",
    "        return torch.stack([torch.tensor(infer(root_id), dtype=torch.float) for root_id in root_ids], dim=0)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_ids):\n",
    "\n",
    "        x = self.encode(x, edge_index, batch_node_ids)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "    def encode(self, x, edge_index, batch_node_ids):\n",
    "        \"\"\"\n",
    "        :param x: 节点特征矩阵\n",
    "        :param edge_index: 边索引\n",
    "        :param batch_node_ids: 当前批次的节点ID\n",
    "        \"\"\"\n",
    "        # 获取当前批次节点的根节点特征\n",
    "        root_embeds = self.root_embedding(batch_node_ids)\n",
    "\n",
    "        # 第一步，聚合邻居特征\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 第二步，计算根节点特征的影响，根节点特征通过线性变换 W2\n",
    "        root_embeds_transformed = self.root_transform(root_embeds.to(device))\n",
    "        root_embeds_transformed = F.relu(root_embeds_transformed)\n",
    "        root_embeds_transformed = F.dropout(root_embeds_transformed, p=0.2, training=self.training)\n",
    "\n",
    "        # 最后，邻居特征加上根节点变换后的特征\n",
    "        x = x + root_embeds_transformed\n",
    "\n",
    "        # 第二层图卷积\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "\n",
    "        root_embeds_transformed2 = self.root_transform2(root_embeds_transformed)\n",
    "        root_embeds_transformed2 = F.relu(root_embeds_transformed2)\n",
    "        root_embeds_transformed2 = F.dropout(root_embeds_transformed2, p=0.2, training=self.training)\n",
    "\n",
    "        x = x + root_embeds_transformed2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model = GNNWithRootEmbedding(64, process_lca_dict, mapp, nodes_vec).to(device)\n",
    "# if not gnnTrain:\n",
    "#     model.load_state_dict(torch.load(gnn_weights))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "l = np.array(labels)\n",
    "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(l),y = l)\n",
    "class_weights = torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "criterion = CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NeighborLoader(graph, num_neighbors=[-1, -1], batch_size=5000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(batch.x, batch.edge_index, batch.n_id)[:batch.batch_size]\n",
    "    y = batch.y[:batch.batch_size]\n",
    "    loss = criterion(predictions, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), batch.batch_size\n",
    "\n",
    "def evaluate_model(batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(batch.x, batch.edge_index, batch.n_id)[:batch.batch_size]\n",
    "        pred_labels = predictions.argmax(dim=1)\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        correct_predictions = int((pred_labels == y).sum())\n",
    "    return correct_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_weights = \"../trained_weights/optc/cmdline_1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0. Training Loss: 0.00011, Accuracy: 0.99682\n",
      "Epoch #1. Training Loss: 0.00011, Accuracy: 0.99744\n",
      "Epoch #2. Training Loss: 0.00011, Accuracy: 0.99651\n",
      "Epoch #3. Training Loss: 0.00012, Accuracy: 0.98878\n",
      "Epoch #4. Training Loss: 0.00011, Accuracy: 0.99368\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    total_loss = total_correct = total_nodes = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        # print(batch.x.shape)\n",
    "        loss, num_nodes = train_model(batch)\n",
    "        total_loss += loss\n",
    "        total_nodes += num_nodes\n",
    "        total_correct += evaluate_model(batch)\n",
    "\n",
    "    average_loss = total_loss / total_nodes\n",
    "    accuracy = total_correct / total_nodes\n",
    "\n",
    "    print(f\"Epoch #{epoch}. Training Loss: {average_loss:.5f}, Accuracy: {accuracy:.5f}\")\n",
    "torch.save(model.state_dict(), gnn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cch_TFE_GNN",
   "language": "python",
   "name": "cch_tfe_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
