{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cch/.conda/envs/cch_TFE_GNN/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "torch.cuda.is_available()  # 如果返回True，说明PyTorch可以看到GPU\n",
    "torch.cuda.device_count()  # 返回GPU的数量\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "# # 获取当前进程，设置进程亲和性（假设你想使用CPU 17）\n",
    "p = psutil.Process(os.getpid())\n",
    "p.cpu_affinity([17])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>actorID</th>\n",
       "      <th>hostname</th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>objectID</th>\n",
       "      <th>pid</th>\n",
       "      <th>ppid</th>\n",
       "      <th>principal</th>\n",
       "      <th>properties</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>actorname</th>\n",
       "      <th>objectname</th>\n",
       "      <th>phrase</th>\n",
       "      <th>command_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>0f0689b0-5339-4791-9751-cf971712f6d2</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>0a980fb9-d7e6-4591-bd67-89b7b2b7962e</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>f1c8b1f4-02d1-4c38-8b2f-15dc06fb49c8</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>84a5970f-27b5-40a8-8570-70382ecbb087</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>6b7c9f39-fd04-4a79-812c-1b40b43a1ae9</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>20f1d8a8-690f-456b-8553-d984028e234d</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>836f85f5-1cc6-40fd-bf48-71867c1524d5</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>4ac9bdec-8b6a-4582-b556-28cee4851ed3</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>8c0f136f-11b1-4db6-a658-fcab280a5d7e</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>af777c15-9352-4f09-a5a7-6ac7a8be924b</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\Device\\HarddiskVolume1\\Windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\Device\\HarddiskVolume1\\Windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    action                               actorID                    hostname  \\\n",
       "0  inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "1  inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "2  inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "3  inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "4  inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "\n",
       "                                     id object  \\\n",
       "0  0f0689b0-5339-4791-9751-cf971712f6d2   FLOW   \n",
       "1  f1c8b1f4-02d1-4c38-8b2f-15dc06fb49c8   FLOW   \n",
       "2  6b7c9f39-fd04-4a79-812c-1b40b43a1ae9   FLOW   \n",
       "3  836f85f5-1cc6-40fd-bf48-71867c1524d5   FLOW   \n",
       "4  8c0f136f-11b1-4db6-a658-fcab280a5d7e   FLOW   \n",
       "\n",
       "                               objectID  pid  ppid  \\\n",
       "0  0a980fb9-d7e6-4591-bd67-89b7b2b7962e  892   544   \n",
       "1  84a5970f-27b5-40a8-8570-70382ecbb087  892   544   \n",
       "2  20f1d8a8-690f-456b-8553-d984028e234d  892   544   \n",
       "3  4ac9bdec-8b6a-4582-b556-28cee4851ed3  892   544   \n",
       "4  af777c15-9352-4f09-a5a7-6ac7a8be924b  892   544   \n",
       "\n",
       "                      principal  \\\n",
       "0  NT AUTHORITY\\NETWORK SERVICE   \n",
       "1  NT AUTHORITY\\NETWORK SERVICE   \n",
       "2  NT AUTHORITY\\NETWORK SERVICE   \n",
       "3  NT AUTHORITY\\NETWORK SERVICE   \n",
       "4  NT AUTHORITY\\NETWORK SERVICE   \n",
       "\n",
       "                                          properties  tid           timestamp  \\\n",
       "0  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "1  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "2  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "3  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "4  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "\n",
       "                                           actorname        objectname  \\\n",
       "0  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "1  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "2  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "3  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "4  \\Device\\HarddiskVolume1\\Windows\\system32\\svcho...  224.0.0.251-5353   \n",
       "\n",
       "                                              phrase command_line  \n",
       "0  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "1  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "2  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "3  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  \n",
       "4  [\\Device\\HarddiskVolume1\\Windows\\system32\\svch...          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# /home/cch/TraceSentinel/data/optc/benign/20-23Sep19/concat_SysClient0051.systemia.com.txt\n",
    "with open(\"../data/optc/benign/20-23Sep19/benign_SysClient0201.systemia.com.txt\", 'r') as file:\n",
    "    content = [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "\n",
    "def Extract_Semantic_Info(event):\n",
    "    object_type = event['object']\n",
    "    properties = event['properties']\n",
    "\n",
    "    label_mapping = {\n",
    "        \"PROCESS\": ('parent_image_path', 'image_path'),\n",
    "        \"FILE\": ('image_path', 'file_path'),\n",
    "        \"FLOW\": ('image_path', 'dest_ip', 'dest_port', 'direction'),\n",
    "        'MODULE': ('image_path', 'module_path')\n",
    "    }\n",
    "\n",
    "    label_keys = label_mapping.get(object_type, None)\n",
    "    if label_keys:\n",
    "        labels = [properties.get(key) for key in label_keys]\n",
    "        # print(labels)\n",
    "        if all(labels):\n",
    "            if object_type==\"PROCESS\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], labels[1]\n",
    "                event[\"command_line\"] = properties.get(\"command_line\", \"None\")\n",
    "                # event[\"command_line\"] = properties.get(\"command_line\", \"None\") if len(properties.get(\"command_line\", \"None\")) > 70 else properties.get(\"command_line\", \"None\")\n",
    "                # event[\"command_line\"] = labels[2] if len(labels[2]) > 70 else labels[2]\n",
    "            elif object_type==\"FILE\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], labels[1]\n",
    "            elif object_type==\"FLOW\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], '-'.join(labels[1:3])\n",
    "                event['action'] = labels[3]\n",
    "            elif object_type == \"MODULE\":\n",
    "                event[\"actorname\"], event[\"objectname\"] = labels[0], labels[1]\n",
    "            else:\n",
    "                pass\n",
    "            return event\n",
    "    return None\n",
    "\n",
    "\n",
    "def Sentence_Construction(entry):\n",
    "    action = entry[\"action\"]\n",
    "    properties = entry['properties']\n",
    "    object_type = entry['object']\n",
    "    phrase=[]\n",
    "    try:\n",
    "        if object_type==\"PROCESS\":\n",
    "            phrase.append(properties.get(\"parent_image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(properties.get(\"command_line\", \"N/A\"))\n",
    "        elif object_type==\"FILE\":\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            phrase.append(properties.get(\"file_path\", \"N/A\"))\n",
    "        elif object_type==\"FLOW\":\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            dest_ip = str(properties.get(\"dest_ip\", \"N/A\"))\n",
    "            dest_port = str(properties.get(\"dest_port\", \"N/A\"))\n",
    "            phrase.append(dest_ip+'-'+dest_port)\n",
    "            # phrase.append(properties.get(\"src_ip\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"src_port\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"dest_ip\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"dest_port\", \"N/A\"))\n",
    "            # phrase.append(properties.get(\"direction\", \"N/A\"))\n",
    "        elif object_type==\"MODULE\":\n",
    "            phrase.append(properties.get(\"image_path\", \"N/A\"))\n",
    "            phrase.append(action)\n",
    "            phrase.append(properties.get(\"module_path\", \"N/A\"))\n",
    "        else:\n",
    "            pass\n",
    "    except KeyError:\n",
    "        phrase = []\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def is_valid_entry(entry):\n",
    "    valid_objects = {'PROCESS', 'FILE', 'FLOW', 'MODULE'}\n",
    "    invalid_actions = {'START', 'TERMINATE', 'OPEN'}\n",
    "\n",
    "    object_valid = entry['object'] in valid_objects\n",
    "    action_valid = entry['action'] not in invalid_actions\n",
    "    actor_object_different = entry['actorID'] != entry['objectID']\n",
    "\n",
    "    return object_valid and action_valid and actor_object_different\n",
    "\n",
    "def Traversal_Rules(data):\n",
    "    filtered_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if is_valid_entry(entry):\n",
    "            filtered_data.append(entry)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def transform(text):\n",
    "    labeled_data = [event for event in (Extract_Semantic_Info(x) for x in text) if event]\n",
    "    data = Traversal_Rules(labeled_data)\n",
    "\n",
    "\n",
    "    phrases = [Sentence_Construction(x) for x in data if Sentence_Construction(x)]\n",
    "    for datum, phrase in zip(data, phrases):\n",
    "        datum['phrase'] = phrase\n",
    "\n",
    "    # data = sorted(data, key=lambda x: x.get('timestamp'))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'].str[:19], format='%Y-%m-%dT%H:%M:%S')\n",
    "    df.sort_values(by='timestamp', inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = transform(content)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_df(df):\n",
    "    # 指定列转换为小写\n",
    "    columns_to_lower = ['action', 'object', 'actorname', 'objectname', 'command_line', 'phrase']\n",
    "    # df[columns_to_lower] = df[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    # 使用apply和lambda函数转换列中的值\n",
    "    df[columns_to_lower] = df[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    # 对'phrase'列中的每个元素（列表中的字符串）应用小写转换\n",
    "    df['phrase'] = df['phrase'].apply(lambda x: [item.lower() if isinstance(item, str) else item for item in x])\n",
    "\n",
    "    node2name = {}\n",
    "    # 首先遍历create事件，提取进程名\n",
    "    extract_process_name = df[(df['object'] == 'process') & (df['action'] == 'create')]\n",
    "    for row in extract_process_name.itertuples():\n",
    "        nodeid = row.objectID\n",
    "        processname = row.objectname\n",
    "        if nodeid not in node2name:\n",
    "            node2name[nodeid] = processname\n",
    "    for row in df[(df['object'] == 'process') & (df['action'] != 'create')].itertuples():\n",
    "        nodeid = row.objectID\n",
    "        processname = row.objectname\n",
    "        if nodeid not in node2name:\n",
    "            node2name[nodeid] = processname\n",
    "    # 使用 map 方法将 node2name 字典中的值应用到objectname列\n",
    "    # fillna(df['objectname']) 保证如果没有对应的 nodeid，在不改变原来的值的情况下保持 objectname 原样\n",
    "    df['objectname'] = df['objectID'].map(node2name).fillna(df['objectname'])\n",
    "    df['actorname'] = df['actorID'].map(node2name).fillna(df['actorname'])\n",
    "\n",
    "\n",
    "    new_path = {}\n",
    "    for row in df[(df['object'] == 'file') & (df['action'] == 'rename')].itertuples():\n",
    "        nodeid = row.objectID\n",
    "        new_pathname = row.properties.get('new_path', 'None')\n",
    "        if nodeid not in new_path:\n",
    "            new_path[nodeid] = new_pathname\n",
    "\n",
    "    df['objectname'] = df['objectID'].map(new_path).fillna(df['objectname'])\n",
    "\n",
    "    # 去除重复事件\n",
    "    df.drop_duplicates(subset=['action', 'actorID', 'objectID', 'object', 'actorname', 'objectname', 'pid', 'ppid'], inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # 根据进程名称和pid生成新的索引\n",
    "    for index, row in df.iterrows():\n",
    "        if row['object'] == 'process':\n",
    "            # 对于 'process' 类型\n",
    "            src_id = stringtomd5(row['actorname'] + str(row['ppid']))\n",
    "            dest_id = stringtomd5(row['objectname'] + str(row['pid']))\n",
    "        elif row['object'] in ['flow', 'file', 'module']:\n",
    "            # 对于 'flow' 或 'file' 类型\n",
    "            src_id = stringtomd5(row['actorname'] + str(row['pid']))\n",
    "            dest_id = stringtomd5(row['objectname'])\n",
    "        else:\n",
    "            # 如果其他类型的 object\n",
    "            src_id = None\n",
    "            dest_id = None\n",
    "        \n",
    "        # 将结果存入新的列\n",
    "        df.at[index, 'src_id'] = src_id\n",
    "        df.at[index, 'dest_id'] = dest_id\n",
    "\n",
    "    # 去除重复事件\n",
    "    df.drop_duplicates(subset=['action', 'src_id', 'dest_id', 'object', 'actorname', 'objectname'], inplace=True)\n",
    "    df = df[df['src_id'] != df['dest_id']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = proprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>actorID</th>\n",
       "      <th>hostname</th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>objectID</th>\n",
       "      <th>pid</th>\n",
       "      <th>ppid</th>\n",
       "      <th>principal</th>\n",
       "      <th>properties</th>\n",
       "      <th>tid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>actorname</th>\n",
       "      <th>objectname</th>\n",
       "      <th>phrase</th>\n",
       "      <th>command_line</th>\n",
       "      <th>src_id</th>\n",
       "      <th>dest_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>0f0689b0-5339-4791-9751-cf971712f6d2</td>\n",
       "      <td>flow</td>\n",
       "      <td>0a980fb9-d7e6-4591-bd67-89b7b2b7962e</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '224.0.0.251'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:39</td>\n",
       "      <td>\\device\\harddiskvolume1\\windows\\system32\\svcho...</td>\n",
       "      <td>224.0.0.251-5353</td>\n",
       "      <td>[\\device\\harddiskvolume1\\windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbb23d203018426c1ccd42dce2cc4c954e14ca79a7dc30...</td>\n",
       "      <td>540a960fe00f02eb2e4f3087a04e3c95767d350cfc8fb5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inbound</td>\n",
       "      <td>4f6f51c2-ff4e-4928-b73f-bb39cdf61b65</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>2d3124d1-10a2-4b7f-9d90-42c468ee3cd4</td>\n",
       "      <td>flow</td>\n",
       "      <td>91842630-88e8-4b58-bcf0-7cb4f65126d4</td>\n",
       "      <td>892</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\NETWORK SERVICE</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': 'ff02::fb', '...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:40</td>\n",
       "      <td>\\device\\harddiskvolume1\\windows\\system32\\svcho...</td>\n",
       "      <td>ff02::fb-5353</td>\n",
       "      <td>[\\device\\harddiskvolume1\\windows\\system32\\svch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbb23d203018426c1ccd42dce2cc4c954e14ca79a7dc30...</td>\n",
       "      <td>e9279d5c69ac1643a5aff74e8c61d79ca87c277c8e44b5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inbound</td>\n",
       "      <td>68891e24-4133-49cd-9551-884fa718579f</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>3ef182a6-3850-452e-9da3-5acef3eb0f69</td>\n",
       "      <td>flow</td>\n",
       "      <td>70b6a611-3725-451b-aeda-3b9b10c0d6fe</td>\n",
       "      <td>2328</td>\n",
       "      <td>1832</td>\n",
       "      <td>NT AUTHORITY\\SYSTEM</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '225.0.0.1', ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:40</td>\n",
       "      <td>\\device\\harddiskvolume1\\python27\\python.exe</td>\n",
       "      <td>225.0.0.1-5000</td>\n",
       "      <td>[\\device\\harddiskvolume1\\python27\\python.exe, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89f52420da1b94201898e86bbec1dca3f43e767ee17713...</td>\n",
       "      <td>2e5184a3ab6979f2edb0508f3e863f7cb8e52a5a538247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>outbound</td>\n",
       "      <td>af5f392e-8aca-4476-b0a8-3e96d72f07c2</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>1385e255-190b-447d-92ee-56f454c0ef70</td>\n",
       "      <td>flow</td>\n",
       "      <td>56860fe9-09a7-4abc-aacf-76ff80e18594</td>\n",
       "      <td>1192</td>\n",
       "      <td>544</td>\n",
       "      <td>NT AUTHORITY\\SYSTEM</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '10.50.5.12',...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:41</td>\n",
       "      <td>\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe</td>\n",
       "      <td>10.50.5.12-9092</td>\n",
       "      <td>[\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>557ff6224594c228cf3b61c5831038405a3509b4ba6364...</td>\n",
       "      <td>0d3542e021e03e483f815f6f5b1fccb2df07ddc18a2a77...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inbound</td>\n",
       "      <td>183db8ca-772a-4edb-86c6-2e76fd9514e5</td>\n",
       "      <td>SysClient0201.systemia.com</td>\n",
       "      <td>dcbb32a3-bc27-4311-842d-d3a3277c018f</td>\n",
       "      <td>flow</td>\n",
       "      <td>ca2e36d9-c576-4823-aefc-affc9b518e87</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NT AUTHORITY\\SYSTEM</td>\n",
       "      <td>{'acuity_level': '1', 'dest_ip': '10.50.255.25...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2019-09-19 17:11:42</td>\n",
       "      <td>system</td>\n",
       "      <td>10.50.255.255-138</td>\n",
       "      <td>[system, inbound, 10.50.255.255-138]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>255baa7d3044f87213df50991bc4999b83bcb83df8de4d...</td>\n",
       "      <td>94db62338644dbea58bfa77a0e6f9c79a25d452d6e6123...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     action                               actorID                    hostname  \\\n",
       "0   inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "1   inbound  4f6f51c2-ff4e-4928-b73f-bb39cdf61b65  SysClient0201.systemia.com   \n",
       "2   inbound  68891e24-4133-49cd-9551-884fa718579f  SysClient0201.systemia.com   \n",
       "3  outbound  af5f392e-8aca-4476-b0a8-3e96d72f07c2  SysClient0201.systemia.com   \n",
       "4   inbound  183db8ca-772a-4edb-86c6-2e76fd9514e5  SysClient0201.systemia.com   \n",
       "\n",
       "                                     id object  \\\n",
       "0  0f0689b0-5339-4791-9751-cf971712f6d2   flow   \n",
       "1  2d3124d1-10a2-4b7f-9d90-42c468ee3cd4   flow   \n",
       "2  3ef182a6-3850-452e-9da3-5acef3eb0f69   flow   \n",
       "3  1385e255-190b-447d-92ee-56f454c0ef70   flow   \n",
       "4  dcbb32a3-bc27-4311-842d-d3a3277c018f   flow   \n",
       "\n",
       "                               objectID   pid  ppid  \\\n",
       "0  0a980fb9-d7e6-4591-bd67-89b7b2b7962e   892   544   \n",
       "1  91842630-88e8-4b58-bcf0-7cb4f65126d4   892   544   \n",
       "2  70b6a611-3725-451b-aeda-3b9b10c0d6fe  2328  1832   \n",
       "3  56860fe9-09a7-4abc-aacf-76ff80e18594  1192   544   \n",
       "4  ca2e36d9-c576-4823-aefc-affc9b518e87     4     0   \n",
       "\n",
       "                      principal  \\\n",
       "0  NT AUTHORITY\\NETWORK SERVICE   \n",
       "1  NT AUTHORITY\\NETWORK SERVICE   \n",
       "2           NT AUTHORITY\\SYSTEM   \n",
       "3           NT AUTHORITY\\SYSTEM   \n",
       "4           NT AUTHORITY\\SYSTEM   \n",
       "\n",
       "                                          properties  tid           timestamp  \\\n",
       "0  {'acuity_level': '1', 'dest_ip': '224.0.0.251'...   -1 2019-09-19 17:11:39   \n",
       "1  {'acuity_level': '1', 'dest_ip': 'ff02::fb', '...   -1 2019-09-19 17:11:40   \n",
       "2  {'acuity_level': '1', 'dest_ip': '225.0.0.1', ...   -1 2019-09-19 17:11:40   \n",
       "3  {'acuity_level': '1', 'dest_ip': '10.50.5.12',...   -1 2019-09-19 17:11:41   \n",
       "4  {'acuity_level': '1', 'dest_ip': '10.50.255.25...   -1 2019-09-19 17:11:42   \n",
       "\n",
       "                                           actorname         objectname  \\\n",
       "0  \\device\\harddiskvolume1\\windows\\system32\\svcho...   224.0.0.251-5353   \n",
       "1  \\device\\harddiskvolume1\\windows\\system32\\svcho...      ff02::fb-5353   \n",
       "2        \\device\\harddiskvolume1\\python27\\python.exe     225.0.0.1-5000   \n",
       "3       \\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe    10.50.5.12-9092   \n",
       "4                                             system  10.50.255.255-138   \n",
       "\n",
       "                                              phrase command_line  \\\n",
       "0  [\\device\\harddiskvolume1\\windows\\system32\\svch...          NaN   \n",
       "1  [\\device\\harddiskvolume1\\windows\\system32\\svch...          NaN   \n",
       "2  [\\device\\harddiskvolume1\\python27\\python.exe, ...          NaN   \n",
       "3  [\\device\\harddiskvolume1\\lwabeat\\\\lwabeat.exe,...          NaN   \n",
       "4               [system, inbound, 10.50.255.255-138]          NaN   \n",
       "\n",
       "                                              src_id  \\\n",
       "0  bbb23d203018426c1ccd42dce2cc4c954e14ca79a7dc30...   \n",
       "1  bbb23d203018426c1ccd42dce2cc4c954e14ca79a7dc30...   \n",
       "2  89f52420da1b94201898e86bbec1dca3f43e767ee17713...   \n",
       "3  557ff6224594c228cf3b61c5831038405a3509b4ba6364...   \n",
       "4  255baa7d3044f87213df50991bc4999b83bcb83df8de4d...   \n",
       "\n",
       "                                             dest_id  \n",
       "0  540a960fe00f02eb2e4f3087a04e3c95767d350cfc8fb5...  \n",
       "1  e9279d5c69ac1643a5aff74e8c61d79ca87c277c8e44b5...  \n",
       "2  2e5184a3ab6979f2edb0508f3e863f7cb8e52a5a538247...  \n",
       "3  0d3542e021e03e483f815f6f5b1fccb2df07ddc18a2a77...  \n",
       "4  94db62338644dbea58bfa77a0e6f9c79a25d452d6e6123...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read      221493\n",
       "modify     21363\n",
       "write      20808\n",
       "create     14592\n",
       "delete     11998\n",
       "rename      2220\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df['object'] == 'file']['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "module     509201\n",
       "file       292474\n",
       "flow       100641\n",
       "process     26133\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['object'] != \"module\"]\n",
    "read_df = df[df['action'] == 'read']\n",
    "write_actions = ['write', 'modify', 'create', 'rename']\n",
    "\n",
    "# 筛选出写操作的行，并排除由 system 和 dwm.exe 修改的文件\n",
    "# write_df = df[(df['action'].isin(write_actions)) & (~df['actorname'].isin(['system', 'dwm.exe']))]\n",
    "sys_dwm = df[df['actorname'].isin(['system', 'dwm.exe'])]['dest_id']\n",
    "\n",
    "write_df = df[df['action'].isin(write_actions)]\n",
    "\n",
    "\n",
    "\n",
    "# write_object_ids = set(write_df['dest_id'])\n",
    "write_object_ids = set(write_df['dest_id']) - set(sys_dwm)\n",
    "readonly_df = read_df[~read_df['dest_id'].isin(write_object_ids)]\n",
    "readonly_df_ids = readonly_df['dest_id']\n",
    "df = df[~df['dest_id'].isin(readonly_df_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从df中去除名为\\device\\harddiskvolume1\\windows\\system32\\svchost.exe和\\device\\harddiskvolume1\\windows\\system32\\conhost.exe的节点\n",
    "exclude_values = [\n",
    "    '\\device\\harddiskvolume1\\windows\\system32\\svchost.exe',\n",
    "    '\\device\\harddiskvolume1\\windows\\system32\\conhost.exe',\n",
    "    'svchost.exe',\n",
    "    'consent.exe'\n",
    "]\n",
    "\n",
    "# 过滤掉包含指定值的行\n",
    "df = df[~df['actorname'].isin(exclude_values) & ~df['objectname'].isin(exclude_values)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到所有 create 操作的父进程和子进程\n",
    "parent_child_df = df[(df['action'] == 'create') & (df['object'] == 'process')]\n",
    "\n",
    "# 提取所有父进程和子进程的 ID\n",
    "parent_ids = set(parent_child_df['src_id'])  # 父进程 ID\n",
    "child_ids = set(parent_child_df['dest_id'])   # 子进程 ID\n",
    "\n",
    "# 所有相关的进程 ID（即有父进程或有子进程的）\n",
    "related_ids = parent_ids.union(child_ids)\n",
    "\n",
    "# 从 df 中移除既没有父进程也没有子进程的节点\n",
    "df = df[df['src_id'].isin(related_ids) | df['dest_id'].isin(related_ids)]\n",
    "\n",
    "# 重置索引\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉没有cmdline的进程节点：\n",
    "df = df[~((df['object'] == 'process') & (df['command_line'] == 'none'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# 假设 df 是你的 DataFrame\n",
    "# 1. 筛选 object 列为 'process' 的行\n",
    "df_process = df[df['object'] == 'process']\n",
    "\n",
    "# 2. 创建一个无向图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 3. 添加边 (src_id, dest_id) 到图中\n",
    "for _, row in df_process.iterrows():\n",
    "    G.add_edge(row['src_id'], row['dest_id'])\n",
    "\n",
    "# 4. 获取图中的所有连通子图\n",
    "connected_components = list(nx.connected_components(G))\n",
    "\n",
    "# 5. 过滤掉节点数小于 2 的子图\n",
    "filtered_components = [comp for comp in connected_components if len(comp) > 2]\n",
    "\n",
    "# 6. 获取所有保留的节点\n",
    "valid_nodes = set(node for component in filtered_components for node in component)\n",
    "\n",
    "# 7. 根据 valid_nodes 过滤原 DataFrame 中的行\n",
    "# df_filtered = df_process[df_process['src_id'].isin(valid_nodes) & df_process['dest_id'].isin(valid_nodes)]\n",
    "\n",
    "# 结果：df_filtered 就是筛选后的 DataFrame\n",
    "df = df[(df['src_id'].isin(valid_nodes) | df['dest_id'].isin(valid_nodes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['object'] != \"module\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file       292474\n",
       "flow       100641\n",
       "process     26133\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_df = df[df['action'] == 'read']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_actions = ['write', 'modify', 'create', 'rename']\n",
    "# write_df = df[df['action'].isin(write_actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_object_ids = set(write_df['dest_id'])\n",
    "# readonly_df = read_df[~read_df['dest_id'].isin(write_object_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readonly_df_ids = readonly_df['dest_id']\n",
    "# df = df[~df['dest_id'].isin(readonly_df_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file       184945\n",
       "flow       100641\n",
       "process     26133\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('../trained_weights/optc/cmdline_attack1_w2v.model')\n",
    "        self.epoch += 1\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "\n",
    "def prepare_sentences(df):\n",
    "    nodes = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for key in ['src_id', 'dest_id']:\n",
    "            node_id = row[key]\n",
    "            nodes.setdefault(node_id, []).extend(row['phrase'])\n",
    "    return list(nodes.values())\n",
    "\n",
    "def train_word2vec_model(df):\n",
    "    phrases = prepare_sentences(df)\n",
    "\n",
    "    logger = EpochLogger()\n",
    "    saver = EpochSaver()\n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=64, window=5, min_count=2, workers=8, epochs=50, callbacks=[saver, logger])\n",
    "\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7c9dc0272490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word2vec_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程见optc_parser3\n",
    "word2vec_weights = '../trained_weights/optc/cmdline_attack1_w2v.model'\n",
    "w2vmodel = Word2Vec.load(word2vec_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inbound     76370\n",
       "create      24169\n",
       "outbound    13482\n",
       "read        12837\n",
       "modify      11172\n",
       "write        9909\n",
       "delete       6822\n",
       "rename       1276\n",
       "Name: action, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['action'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def infer(word):\n",
    "    if word in  w2vmodel.wv:\n",
    "        word_embeddings = w2vmodel.wv[word]\n",
    "    else:\n",
    "        word_embeddings = np.zeros(64)\n",
    "\n",
    "    output_embedding = torch.tensor(word_embeddings, dtype=torch.float)\n",
    "    output_embedding = output_embedding.detach().cpu().numpy()\n",
    "    return output_embedding\n",
    "\n",
    "\n",
    "def Featurize(df):\n",
    "    # dummies = {'process': 0, 'flow': 1, 'file': 2, 'module': 3}\n",
    "    dummies = {'process': 0, 'flow': 1, 'file': 2}\n",
    "\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    lblmap = {}\n",
    "    neimap = {}\n",
    "    edges = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        actor_id, object_id = row['src_id'], row[\"dest_id\"]\n",
    "        object_type = row['object']\n",
    "\n",
    "        if actor_id not in nodes:\n",
    "            nodes[actor_id] = row['actorname']\n",
    "        if object_id not in nodes:\n",
    "            nodes[object_id] = row['objectname']\n",
    "\n",
    "        # nodes.setdefault(actor_id, []).extend(row['phrase'])\n",
    "        # nodes.setdefault(object_id, []).extend(row['phrase'])\n",
    "\n",
    "        labels[actor_id] = dummies.get('process', -1)\n",
    "        labels[object_id] = dummies.get(object_type, -1)\n",
    "\n",
    "        if actor_id not in lblmap:\n",
    "            lblmap[actor_id] = row['actorname']\n",
    "        if object_id not in lblmap:\n",
    "            lblmap[object_id] = row['objectname']\n",
    "\n",
    "        neimap.setdefault(actor_id, set()).add(row['objectname'])\n",
    "        neimap.setdefault(object_id, set()).add(row['actorname'])\n",
    "\n",
    "        edge_type = row['action']\n",
    "        edges.append((actor_id, object_id, edge_type))\n",
    "\n",
    "    features, feat_labels, edge_index = [], [], [[], []]\n",
    "    node_index = {}\n",
    "\n",
    "    for node, nodename in nodes.items():\n",
    "        features.append(infer(nodename))\n",
    "        feat_labels.append(labels[node])\n",
    "        node_index[node] = len(features) - 1\n",
    "\n",
    "    for src, dst, action in edges:\n",
    "        if action=='read' or action=='inbound':\n",
    "            edge_index[0].append(node_index[dst])\n",
    "            edge_index[1].append(node_index[src])\n",
    "        elif action=='create':\n",
    "            edge_index[0].append(node_index[src])\n",
    "            edge_index[1].append(node_index[dst])\n",
    "            edge_index[0].append(node_index[dst])\n",
    "            edge_index[1].append(node_index[src])\n",
    "        else:\n",
    "            edge_index[0].append(node_index[src])\n",
    "            edge_index[1].append(node_index[dst])\n",
    "\n",
    "    mapp = list(node_index.keys())\n",
    "\n",
    "    return features, np.array(feat_labels), edge_index, mapp, lblmap, neimap\n",
    "\n",
    "nodes,labels,edges,mapp,lbl,nemap = Featurize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49337, 49337, 49337)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes),len(labels),len(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个字典，存储每个节点对应的特征向量\n",
    "nodes_vec = {}\n",
    "for i, j in zip(mapp, nodes):\n",
    "    nodes_vec[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交换源节点和目标节点，因为信息汇聚是由源节点汇聚到目标节点\n",
    "# edges[0], edges[1] = edges[1], edges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sc  config onesyncsvc start=disabled'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['object'] == 'process']['command_line'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.4708254 , -1.2484082 , -0.69898814,  0.23804194,  0.09547319,\n",
       "        0.3872281 ,  0.3222387 ,  0.24613985,  0.5577434 ,  0.1049924 ,\n",
       "       -2.3596153 , -0.24298401,  2.2925646 , -2.7819586 ,  4.5058675 ,\n",
       "        1.3748513 , -0.32721066, -1.9327581 , -0.03271226, -2.381833  ,\n",
       "        5.8508625 , -0.4474114 ,  0.841744  , -0.51245177,  2.1129937 ,\n",
       "       -4.5723267 , -1.8549986 ,  1.173516  ,  1.1397079 , -2.5069451 ,\n",
       "        1.2083439 , -0.13087787, -2.596347  , -1.7488282 ,  1.1060201 ,\n",
       "        1.3620267 ,  1.7750661 , -6.8589807 , -3.3996022 , -1.8615474 ,\n",
       "       -2.5184643 ,  1.6608161 ,  3.5593877 , -4.9322257 ,  2.0139499 ,\n",
       "       -0.76103675, -0.8358026 , -3.5051167 ,  0.81226474, -0.8011651 ,\n",
       "       -2.387015  ,  1.5145344 ,  0.28557062,  0.3697181 , -1.4696822 ,\n",
       "       -0.3772158 , -2.8052268 , -0.8242783 ,  2.6637394 , -4.3633223 ,\n",
       "       -2.763947  ,  1.2412436 , -0.89170784,  0.5798774 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('schtasks  /query /tn \"kickoffautologinrandom\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 从df中去除名为\\device\\harddiskvolume1\\windows\\system32\\svchost.exe和\\device\\harddiskvolume1\\windows\\system32\\conhost.exe的节点\n",
    "# exclude_values = [\n",
    "#     '\\device\\harddiskvolume1\\windows\\system32\\svchost.exe',\n",
    "#     '\\device\\harddiskvolume1\\windows\\system32\\conhost.exe',\n",
    "#     'svchost.exe',\n",
    "#     'consent.exe'\n",
    "# ]\n",
    "\n",
    "# # 过滤掉包含指定值的行\n",
    "# df = df[~df['actorname'].isin(exclude_values) & ~df['objectname'].isin(exclude_values)]\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lowest_ancestor_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_process_lineage = df[(df['action'] == 'create') & (df['object'] == 'process')]\n",
    "\n",
    "# 记录每个进程的父进程及其时间戳和 cmdline 信息\n",
    "process_parents = {}\n",
    "\n",
    "# 进程谱系树生成\n",
    "import networkx as nx\n",
    "\n",
    "g = nx.MultiDiGraph()\n",
    "node_map = {}\n",
    "# count_node = 0\n",
    "for index, row in df_process_lineage.iterrows():\n",
    "    src, source_name, dst, dstname, etype, object_type = row['src_id'], row[\"actorname\"],  row['dest_id'], row['objectname'], row['action'], row['object']\n",
    "    command_line = row['command_line']\n",
    "    if src not in node_map:\n",
    "        node_map[src] = src\n",
    "        g.add_node(src, type=\"process\", pname=source_name)\n",
    "    \n",
    "    if dst not in node_map:\n",
    "        node_map[dst] = dst\n",
    "        if source_name==dstname:\n",
    "            g.add_node(dst, type=object_type, pname=dstname)\n",
    "        else:\n",
    "            g.add_node(dst, type=object_type, pname=dstname)\n",
    "    \n",
    "    if g.has_edge(node_map[dst], node_map[src]):\n",
    "        pass\n",
    "    else:\n",
    "        if g.has_edge(node_map[src], node_map[dst]):\n",
    "            pass\n",
    "        elif etype=='create':\n",
    "            g.add_edge(node_map[src], node_map[dst], type=etype, commandline=command_line)\n",
    "        else:\n",
    "            pass\n",
    "    if dst not in process_parents:\n",
    "        process_parents[dst] = []\n",
    "    process_parents[dst].append((src, command_line))\n",
    "\n",
    "def find_lowest_common_ancestor(graph, target_pid):\n",
    "    \"\"\"\n",
    "    查找某个进程的具有多个子进程的最低祖先。\n",
    "    :param graph: NetworkX图对象\n",
    "    :param target_pid: 目标进程ID\n",
    "    :return: 具有多个子进程的最低祖先进程ID\n",
    "    \"\"\"\n",
    "    # 获取目标进程的父进程\n",
    "    def get_parent(pid):\n",
    "        parents = process_parents.get(pid, [])\n",
    "        if parents:\n",
    "            return parents[0][0]  # 假设每个进程有一个父进程\n",
    "        return None\n",
    "    \n",
    "    # 遍历并查找具有多个子进程的父进程\n",
    "    def has_multiple_children(pid):\n",
    "        children = list(graph.successors(pid))\n",
    "        return len(children) > 1  # 如果父进程有多个子进程\n",
    "    \n",
    "    # 从目标进程开始，逐步向上遍历父进程\n",
    "    current_pid = target_pid\n",
    "    \n",
    "    while True:\n",
    "        parent_pid = get_parent(current_pid)\n",
    "        if parent_pid is None:\n",
    "            return None  # 找不到父进程，说明已经是根节点\n",
    "        if has_multiple_children(parent_pid):\n",
    "            # return parent_pid\n",
    "            return current_pid  # 找到具有多个子进程的最低祖先\n",
    "        current_pid = parent_pid  # 向上查找父进程\n",
    "\n",
    "# 存储每个进程的最低祖先\n",
    "process_lca_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "# 遍历所有进程，查找其最低祖先并存储\n",
    "for node in g.nodes:\n",
    "    lca = find_lowest_common_ancestor(g, node)\n",
    "    if lca is not None:\n",
    "        process_lca_dict[node] = process_parents.get(lca, 'meiyou')[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9303"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49's lowest ancestor with multiple children: c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49\n"
     ]
    }
   ],
   "source": [
    "# 示例使用\n",
    "process_id = 'c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49'  # 例如，查找进程 '1006' 的最低祖先\n",
    "ancestor = find_lowest_common_ancestor(g, process_id)\n",
    "if ancestor:\n",
    "    print(f\"Process {process_id}'s lowest ancestor with multiple children: {ancestor}\")\n",
    "else:\n",
    "    print(f\"Process {process_id} has no ancestor with multiple children.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\windows\\system32\\cmd.exe /c \"tasklist\"\n"
     ]
    }
   ],
   "source": [
    "print(process_lca_dict.get('c3c2ee2eb8bc61dce7bb2075785780f97ccdbf170f374281cceec8b4c2e5be49'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果一个进程没有父进程，那么他的根节点是open他的进程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(process_lca_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GNNWithRootEmbedding(nn.Module):\n",
    "    def __init__(self, num_features, process_lca_dict, node_id_to_hash, node_hash_to_vec):\n",
    "        super(GNNWithRootEmbedding, self).__init__()\n",
    "        \n",
    "        self.node_hash_to_vec = node_hash_to_vec # 存储节点的uuid到其初始向量的映射，其中uuid是hash值\n",
    "        self.node_id_to_hash = node_id_to_hash # 存储节点的batch.n_id到节点uuid的映射，其中batch.n_id的数值，例如1，2，3，其中uuid是hash值\n",
    "        self.process_lca_dict = process_lca_dict  # 存储节点与其根节点ID的映射\n",
    "        self.num_features = num_features  # 节点特征维度\n",
    "        \n",
    "        # 第一层图卷积，输入维度为特征 + 根节点特征维度\n",
    "        self.conv1 = SAGEConv(num_features, 128, normalize=True)  # 用于邻居节点特征的聚合\n",
    "        # 第二层图卷积\n",
    "        self.conv2 = SAGEConv(128, 64, normalize=True)\n",
    "        \n",
    "        # 用于根节点特征的变换矩阵W2\n",
    "        self.root_transform = nn.Linear(num_features, 128)\n",
    "        # 用于根节点特征的变换矩阵W3\n",
    "        self.root_transform2 = nn.Linear(128, 64)\n",
    "\n",
    "        self.linear = nn.Linear(64, 3)\n",
    "    \n",
    "    def root_embedding(self, node_ids):\n",
    "        \"\"\"\n",
    "        通过根节点映射字典，直接获取根节点的特征。\n",
    "        :param node_ids: 当前批次中的节点ID\n",
    "        :return: 返回每个节点对应的根节点嵌入特征\n",
    "        \"\"\"\n",
    "        # 获取节点对应的hash值\n",
    "        node_hash = [self.node_id_to_hash[node_id] for node_id in node_ids]\n",
    "        # 获取节点对应的根节点特征（通过process_lca_dict获得根节点ID）\n",
    "        root_ids = [self.process_lca_dict.get(node_id, \"meiyougengjiedian\") for node_id in node_hash]\n",
    "        \n",
    "        # 在这里你可以调用任意方法返回根节点的特征\n",
    "        # 假设根节点特征向量已经通过某种方式计算并存储，返回对应的特征\n",
    "        # get_origin_embedding 函数假设返回根节点的嵌入特征\n",
    "        # return torch.stack([torch.tensor(self.node_hash_to_vec.get(root_id, torch.zeros(self.num_features))) for root_id in root_ids], dim=0)\n",
    "        return torch.stack([torch.tensor(infer(root_id), dtype=torch.float) for root_id in root_ids], dim=0)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_ids):\n",
    "\n",
    "        x = self.encode(x, edge_index, batch_node_ids)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "    def encode(self, x, edge_index, batch_node_ids):\n",
    "        \"\"\"\n",
    "        :param x: 节点特征矩阵\n",
    "        :param edge_index: 边索引\n",
    "        :param batch_node_ids: 当前批次的节点ID\n",
    "        \"\"\"\n",
    "        # 获取当前批次节点的根节点特征\n",
    "        root_embeds = self.root_embedding(batch_node_ids)\n",
    "\n",
    "        # 第一步，聚合邻居特征\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 第二步，计算根节点特征的影响，根节点特征通过线性变换 W2\n",
    "        root_embeds_transformed = self.root_transform(root_embeds.to(device))\n",
    "        root_embeds_transformed = F.relu(root_embeds_transformed)\n",
    "        root_embeds_transformed = F.dropout(root_embeds_transformed, p=0.2, training=self.training)\n",
    "\n",
    "        # 最后，邻居特征加上根节点变换后的特征\n",
    "        x = x + root_embeds_transformed\n",
    "\n",
    "        # 第二层图卷积\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "\n",
    "        root_embeds_transformed2 = self.root_transform2(root_embeds_transformed)\n",
    "        root_embeds_transformed2 = F.relu(root_embeds_transformed2)\n",
    "        root_embeds_transformed2 = F.dropout(root_embeds_transformed2, p=0.2, training=self.training)\n",
    "\n",
    "        x = x + root_embeds_transformed2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model = GNNWithRootEmbedding(64, process_lca_dict, mapp, nodes_vec).to(device)\n",
    "# if not gnnTrain:\n",
    "#     model.load_state_dict(torch.load(gnn_weights))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "l = np.array(labels)\n",
    "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(l),y = l)\n",
    "class_weights = torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "criterion = CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NeighborLoader(graph, num_neighbors=[-1, -1], batch_size=5000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(batch.x, batch.edge_index, batch.n_id)[:batch.batch_size]\n",
    "    y = batch.y[:batch.batch_size]\n",
    "    loss = criterion(predictions, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), batch.batch_size\n",
    "\n",
    "def evaluate_model(batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(batch.x, batch.edge_index, batch.n_id)[:batch.batch_size]\n",
    "        pred_labels = predictions.argmax(dim=1)\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        correct_predictions = int((pred_labels == y).sum())\n",
    "    return correct_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_weights = \"../trained_weights/optc/cmdline_attack1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0. Training Loss: 0.00018, Accuracy: 0.90569\n",
      "Epoch #1. Training Loss: 0.00014, Accuracy: 0.95142\n",
      "Epoch #2. Training Loss: 0.00013, Accuracy: 0.95348\n",
      "Epoch #3. Training Loss: 0.00013, Accuracy: 0.95604\n",
      "Epoch #4. Training Loss: 0.00013, Accuracy: 0.95857\n",
      "Epoch #5. Training Loss: 0.00012, Accuracy: 0.96341\n",
      "Epoch #6. Training Loss: 0.00012, Accuracy: 0.96425\n",
      "Epoch #7. Training Loss: 0.00012, Accuracy: 0.97144\n",
      "Epoch #8. Training Loss: 0.00012, Accuracy: 0.97886\n",
      "Epoch #9. Training Loss: 0.00012, Accuracy: 0.98376\n",
      "Epoch #10. Training Loss: 0.00012, Accuracy: 0.98638\n",
      "Epoch #11. Training Loss: 0.00012, Accuracy: 0.98922\n",
      "Epoch #12. Training Loss: 0.00012, Accuracy: 0.98944\n",
      "Epoch #13. Training Loss: 0.00012, Accuracy: 0.95711\n",
      "Epoch #14. Training Loss: 0.00012, Accuracy: 0.96368\n",
      "Epoch #15. Training Loss: 0.00012, Accuracy: 0.96826\n",
      "Epoch #16. Training Loss: 0.00012, Accuracy: 0.97154\n",
      "Epoch #17. Training Loss: 0.00012, Accuracy: 0.97580\n",
      "Epoch #18. Training Loss: 0.00012, Accuracy: 0.98381\n",
      "Epoch #19. Training Loss: 0.00012, Accuracy: 0.97633\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    total_loss = total_correct = total_nodes = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        # print(batch.x.shape)\n",
    "        loss, num_nodes = train_model(batch)\n",
    "        total_loss += loss\n",
    "        total_nodes += num_nodes\n",
    "        total_correct += evaluate_model(batch)\n",
    "\n",
    "    average_loss = total_loss / total_nodes\n",
    "    accuracy = total_correct / total_nodes\n",
    "\n",
    "    print(f\"Epoch #{epoch}. Training Loss: {average_loss:.5f}, Accuracy: {accuracy:.5f}\")\n",
    "torch.save(model.state_dict(), gnn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cch_TFE_GNN",
   "language": "python",
   "name": "cch_tfe_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
